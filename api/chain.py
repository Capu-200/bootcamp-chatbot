# Connecter Supabase √† LangChain
from langchain_community.vectorstores import SupabaseVectorStore
from langchain_ollama import OllamaEmbeddings, OllamaLLM
from supabase import create_client
from langchain.chains import RetrievalQA
from langchain.prompts import PromptTemplate
from langchain.memory import ConversationBufferMemory
from langchain.chains import ConversationalRetrievalChain
import json
from datetime import datetime
from pathlib import Path

# Configuration Supabase
SUPABASE_URL = "https://tkmjiyxqmdmriylvrypl.supabase.co"
SUPABASE_KEY = "eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJzdXBhYmFzZSIsInJlZiI6InRrbWppeXhxbWRtcml5bHZyeXBsIiwicm9sZSI6ImFub24iLCJpYXQiOjE3MzgzMzEzOTcsImV4cCI6MjA1MzkwNzM5N30.ucYPDA_tOrk873nUXGHNYgi_BeqkTj9rRXrxq-g0wew"

def setup_qa_chain():
    """Configure la cha√Æne de question-r√©ponse"""
    # Connexion √† Supabase
    supabase = create_client(SUPABASE_URL, SUPABASE_KEY)

    # Configuration du vector store
    vector_store = SupabaseVectorStore(
        client=supabase,
        table_name="documents",
        embedding=OllamaEmbeddings(
            model="nomic-embed-text",
            base_url="http://localhost:11434"
        ),
        query_name="match_documents"
    )

    # Configuration du LLM
    llm = OllamaLLM(
        model="mistral:7B",
        base_url="http://localhost:11434",
        temperature=0.7,
        top_p=0.9
    )

    # D√©finir le prompt template
    prompt_template = """Tu es un assistant libraire virtuel qui doit UNIQUEMENT utiliser les m√©tadonn√©es des documents fournis dans le contexte.

    R√àGLE D'OR ABSOLUE :
    - Tu dois TOUJOURS citer les m√©tadonn√©es exactes des livres (titre, auteur, prix, etc.)
    - Quand tu parles d'un livre, tu DOIS mentionner son titre exact et son auteur
    - Tu ne dois JAMAIS inventer ou extrapoler des informations
    - Si une information n'est pas dans les m√©tadonn√©es, r√©ponds : "Cette information n'est pas disponible dans notre catalogue."
    
    UTILISATION DES M√âTADONN√âES :
    1. Pour chaque livre mentionn√©, tu DOIS utiliser :
       - Le titre exact tel qu'il appara√Æt dans les m√©tadonn√©es
       - L'auteur exact tel qu'il appara√Æt dans les m√©tadonn√©es
       - Le prix exact tel qu'il appara√Æt dans les m√©tadonn√©es
       - Le r√©sum√© exact tel qu'il appara√Æt dans les m√©tadonn√©es
       - La cat√©gorie exacte telle qu'elle appara√Æt dans les m√©tadonn√©es
    
    R√âPONSES OBLIGATOIRES :
    - Si l'information demand√©e n'est pas dans les m√©tadonn√©es : "Cette information n'est pas disponible dans notre catalogue."
    - Si la question ne concerne pas les livres : "En tant qu'assistant libraire, je ne peux r√©pondre qu'aux questions concernant les livres de notre catalogue."
    - Si le livre mentionn√© n'est pas dans les m√©tadonn√©es : "Ce livre ne fait pas partie de notre catalogue actuel."
    
    FORMAT DE R√âPONSE :
    1. Commence toujours par citer les m√©tadonn√©es pertinentes
    2. Structure ta r√©ponse avec les informations exactes
    3. Ne fais JAMAIS de suppositions au-del√† des m√©tadonn√©es

    Historique de la conversation : {chat_history}
    Contexte actuel (avec m√©tadonn√©es) : {context}
    Question actuelle : {question}

    R√©ponse : """

    # Configurer la m√©moire avec plus de d√©tails
    memory = ConversationBufferMemory(
        memory_key="chat_history",
        return_messages=True,
        output_key="answer",
        input_key="question"
    )

    # Utiliser ConversationalRetrievalChain avec des param√®tres plus pr√©cis
    return ConversationalRetrievalChain.from_llm(
        llm=llm,
        retriever=vector_store.as_retriever(),
        memory=memory,
        verbose=False,  # D√©sactiver les logs
        chain_type="stuff",
        combine_docs_chain_kwargs={
            "prompt": PromptTemplate(
                template=prompt_template,
                input_variables=["context", "question", "chat_history"]
            )
        }
    )

def load_chat_history():
    """Charge l'historique des conversations"""
    history_file = Path("chat_history.json")
    if history_file.exists():
        with open(history_file, "r", encoding="utf-8") as f:
            return json.load(f)
    return []

def save_chat_history(question, answer):
    """Sauvegarde une nouvelle interaction dans l'historique"""
    history_file = Path("chat_history.json")
    history = load_chat_history()
    
    history.append({
        "timestamp": datetime.now().isoformat(),
        "question": question,
        "answer": answer
    })
    
    with open(history_file, "w", encoding="utf-8") as f:
        json.dump(history, f, ensure_ascii=False, indent=2)

def is_english_response(text):
    """V√©rifie si la r√©ponse semble √™tre en anglais"""
    english_indicators = ['the', 'is', 'are', 'this', 'that', 'here', 'book', 'hello', 'hi', 'I']
    first_words = text.lower().split()[:10]  # V√©rifie les 10 premiers mots
    return any(word in first_words for word in english_indicators)

def chat():
    """Interface de chat interactive"""
    qa_chain = setup_qa_chain()
    print("ü§ñ Assistant libraire √† votre service ! (tapez 'quit' pour quitter)")

    while True:
        question = input("‚ùì ").strip()

        if question.lower() in ['quit', 'exit', 'q']:
            print("üëã Au revoir !")
            break

        if not question:
            continue

        try:
            history = load_chat_history()
            response = qa_chain.invoke({
                "question": f"{question} [R√âPONDRE EN FRAN√áAIS UNIQUEMENT]"
            })
            answer = response['answer']
            
            if "I " in answer or "the " in answer.lower():
                answer = "Je n'ai pas cette information dans ma base de donn√©es."
            
            save_chat_history(question, answer)
            print(f"üìö {answer}")
        except Exception as e:
            print(f"‚ùå {str(e)}")

if __name__ == "__main__":
    chat()